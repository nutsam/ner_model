import re
import spacy
from pprint import pprint
from collections import defaultdict
from ckip_transformers.nlp import CkipNerChunker

class TextPreprocessor:
    def __init__(self, stopwords=None, remove_punctuation=True):
        self.stopwords = stopwords or set()
        self.remove_punctuation = remove_punctuation
    
    def preprocess(self, text):
        """
        é€²è¡Œæ–‡æœ¬å‰è™•ç†ï¼š
        1. ç§»é™¤ç¶²å€
        2. ç§»é™¤ HTML æ¨™ç±¤
        3. ç§»é™¤ç‰¹æ®Šå­—å…ƒï¼ˆå¯é¸æ˜¯å¦ä¿ç•™æ¨™é»ç¬¦è™Ÿï¼‰
        4. çµ±ä¸€ç©ºç™½
        5. ç§»é™¤åœç”¨è©
        """
        text = re.sub(r'(http|ftp|https)://[^\s]+', '', text)  # ç§»é™¤ç¶²å€
        text = re.sub(r'<[^>]*>', '', text)  # ç§»é™¤ HTML æ¨™ç±¤
        
        if self.remove_punctuation:
            text = re.sub(r'[^\u4e00-\u9fffA-Za-z0-9\s]', '', text)
        
        text = re.sub(r'\s+', ' ', text).strip()  # çµ±ä¸€ç©ºç™½
        
        if self.stopwords:
            text = ' '.join(word for word in text.split() if word not in self.stopwords)
        
        return text

class TextMasker:
    @staticmethod
    def replace_chinese_with_underscores(text):
        """å°‡ä¸­æ–‡æ›¿æ›ç‚ºç›¸åŒé•·åº¦çš„åº•ç·š"""
        pattern = re.compile(r'[\u4e00-\u9fff0-9]+')
        return pattern.sub(lambda match: '_' * len(match.group(0)), text).replace('\n', '')

    @staticmethod
    def replace_english_with_underscores(text):
        """å°‡è‹±æ–‡å–®å­—æ›¿æ›ç‚ºç›¸åŒé•·åº¦çš„åº•ç·š"""
        pattern = re.compile(r'[A-Za-z]+')
        return pattern.sub(lambda match: '_' * len(match.group(0)), text).replace('\n', '')

class NamedEntityRecognizer:
    def __init__(self):
        self.ner_types = {"CARDINAL", "DATE", "EVENT", "FAC", "GPE", "LANGUAGE", "LAW",
                          "LOC", "MONEY", "NORP", "ORDINAL", "ORG", "PERCENT", "PERSON",
                          "PRODUCT", "QUANTITY", "TIME", "WORK_OF_ART"}
        
        # è¼‰å…¥ spaCy è‹±æ–‡æ¨¡å‹
        self.nlp_en = spacy.load("en_core_web_sm")
        self.nlp_en.add_pipe("gliner_spacy", config={"labels": list(self.ner_types)})
        
        # è¼‰å…¥ CKIP ä¸­æ–‡æ¨¡å‹
        self.nlp_zh = CkipNerChunker(model="bert-base")

    def recognize_english(self, text):
        doc = self.nlp_en(text)
        ner_dict = {ner: [] for ner in self.ner_types}
        
        for ent in doc.ents:
            cleaned_text = ent.text.strip().strip('_')
            if cleaned_text and ent.text == cleaned_text and ent.label_ in self.ner_types:
                ner_dict[ent.label_].append(cleaned_text)
        
        return ner_dict

    def recognize_chinese(self, text):
        result = self.nlp_zh([text])
        ner_dict = {ner: [] for ner in self.ner_types}
        
        for token in result[0]:
            cleaned_word = token.word.strip().strip('_')
            if cleaned_word and token.word == cleaned_word:
                ner_dict[token.ner].append(cleaned_word)
        
        return ner_dict

    def merge_results(self, ner_en_dict, ner_zh_dict):
        merged_dict = defaultdict(list)
        for d in [ner_en_dict, ner_zh_dict]:
            for key, value in d.items():
                merged_dict[key].extend(value)
        return dict(merged_dict)

if __name__ == "__main__":
    text = """
        ğŸ“¢ Breaking News | é‡å¤§æ¶ˆæ¯ï¼ ğŸ‰

        ğŸ“… æ˜¨å¤©ï¼ˆğŸ®ğŸ¬ğŸ®ğŸ±/ğŸ¬ğŸ®/ğŸ®ğŸ¬ï¼‰ï¼Œå°ç£ğŸ‡¹ğŸ‡¼ç¸½çµ± æå®‰ç„¶ (An-Ran Li) åœ¨å°åŒ—101 ğŸ™ èˆ‰è¡Œè¨˜è€…æœƒï¼Œå®£å¸ƒæ”¿åºœå°‡æŠ•è³‡ NT$1ï¸âƒ£5ï¸âƒ£0ï¸âƒ£ å„„ ç”¨æ–¼ AI ç™¼å±•è¨ˆç•«ã€‚è©²è¨ˆç•«ç”± è¡Œæ”¿é™¢ç§‘æŠ€éƒ¨ (MOST) è² è²¬ï¼Œé è¨ˆæ–¼ ğŸ®ğŸ¬ğŸ®ğŸ² å¹´é–‹å§‹å¯¦æ–½ã€‚

        ğŸŒ åœ‹éš›ä¼æ¥­å¤§æˆ° AI é ˜åŸŸï¼ åœ¨çŸ½è°· ğŸŒï¼ŒMetaã€Google DeepMindã€ğ•ºğ–•ğ–Šğ–“ğ•¬ğ– å’Œ é˜¿é‡Œå·´å·´ å®£å¸ƒå»ºç«‹ã€Œğ“ğ“˜ ğ“¡ğ“®ğ“¼ğ“®ğ“ªğ“»ğ“¬ğ“± ğ“’ğ“¸ğ“µğ“µğ“ªğ“«ğ“¸ğ“»ğ“ªğ“½ğ“²ğ“¸ğ“·ã€ï¼Œå…±åŒç ”ç©¶å¤§å‹èªè¨€æ¨¡å‹ (LLM) çš„å¯è§£é‡‹æ€§èˆ‡å…¬å¹³æ€§ã€‚åŒæ™‚ï¼Œæ—¥æœ¬ğŸ‡¯ğŸ‡µçš„ SONY ä¹Ÿå®£å¸ƒæŠ•å…¥ Â¥500 å„„ ç”¨æ–¼ AI æ™¶ç‰‡é–‹ç™¼ã€‚

        ğŸ® 2025 å¹´é›»ç«¶ç•Œç››äº‹ï¼ ä»Šå¹´ 7 æœˆï¼Œè‹±é›„è¯ç›Ÿ (LoL) ä¸–ç•Œå¤§è³½ (Worlds 2025) å°‡æ–¼ ç¾åœ‹ğŸ†æ´›æ‰ç£¯ Crypto.com Arena èˆ‰è¾¦ï¼Œä¾†è‡ª åŒ—ç¾ (LCS)ã€æ­æ´² (LEC)ã€éŸ“åœ‹ (LCK) å’Œä¸­åœ‹ (LPL) çš„æˆ°éšŠå°‡çˆ­å¥ªå† è»ğŸ…ã€‚æ ¹æ“šæ•¸æ“šåˆ†æï¼Œå»å¹´ LCK ç¸½æ”¶å…¥é” $75.3Mï¼Œè€Œ LPL å‰‡é«˜é” Â¥5.8Bã€‚

        ğŸ’¸ å…¨çƒå¸‚å ´è¶¨å‹¢ æ ¹æ“š Bloomberg çš„æœ€æ–°å ±å‘Šï¼š

        2024 å¹´å…¨çƒ AI ç”¢æ¥­ä¼°å€¼é” $ğŸ®ğŸğŸ¬B USD ğŸ’°ï¼ŒåŒæ¯”å¢é•· 35.2% ğŸ“ˆã€‚
        ğ’•ğ’‰ğ’† S&P 500 æŒ‡æ•¸çªç ´ 5,ğŸ¬ğŸ¬ğŸ¬ é»ï¼Œå‰µä¸‹æ­·å²æ–°é«˜ğŸ¦ï¼
        2025 å¹´ï¼Œäºé¦¬éœ (Amazon) é è¨ˆæŠ•å…¥ â‚¬10B æ–¼é›²ç«¯ AI è¨ˆç•« â˜ï¸ã€‚
        ğŸ­ æ–‡åŒ–èˆ‡å¨›æ¨‚ Netflix è¿‘æ—¥æ¨å‡ºäº†ä¸€éƒ¨æ–°åŠ‡ã€Šğ•¯ğ–ğ–Œğ–ğ–™ğ–†ğ–‘ ğ•¯ğ–—ğ–Šğ–†ğ–’ğ–˜ã€‹ï¼Œè©²åŠ‡è¬›è¿°äº†ä¸€å AI å·¥ç¨‹å¸«åœ¨è™›æ“¬ä¸–ç•Œèˆ‡ç¾å¯¦ä¹‹é–“ç©¿æ¢­çš„æ•…äº‹ğŸ“½ã€‚è©•è«–å®¶èªç‚ºï¼šã€Œé€™éƒ¨åŠ‡å±•ç¤ºäº† 21 ä¸–ç´€æ•¸ä½åŒ–èˆ‡ AI æŠ€è¡“çš„æœªä¾†ã€‚ã€

        ğŸ’» è©­ç•°çš„ AI éŒ¯èª¤ï¼Ÿ ä¸€ä½ Twitter ç”¨æˆ¶ @ai_errorğŸ’¥ æŒ‡å‡ºï¼ŒChatGPT æ–¼ ğŸ®ğŸ¬ğŸ®ğŸ° å¹´ ç”Ÿæˆäº†ä¸€ç¯‡ä¸å­˜åœ¨çš„æ–°èï¼Œå°è‡´å¤§é‡ç¶²å‹èª¤ä»¥ç‚ºæ—¥æœ¬æ”¿åºœå€’é–‰ (å‡çš„ï¼)ã€‚é€™ä¹Ÿè®“ OpenAI å®£å¸ƒå°‡å¼·åŒ– AI å…§å®¹çœŸå¯¦æ€§é©—è­‰ã€‚

        ğŸ“¢ è¯çµ¡æˆ‘å€‘ æƒ³äº†è§£æ›´å¤šè©³æƒ…ï¼Ÿè«‹è¨ªå•ï¼š

        ğŸŒ https://www.aitechnews.com/?ref=testğŸ”—
        ğŸ“§ contact@ai-news.com
        â˜ï¸ +1-(800)-555-ğŸ“-AI24
    """
    
    # æ–‡æœ¬å‰è™•ç†
    preprocessor = TextPreprocessor(remove_punctuation=False)
    cleaned_text = preprocessor.preprocess(text)
    print(cleaned_text)
    print('- '* 50)
    
    # æ–‡å­—é®ç½©
    english_masked = TextMasker.replace_chinese_with_underscores(cleaned_text)
    chinese_masked = TextMasker.replace_english_with_underscores(cleaned_text)
    
    print(english_masked)
    print('- '* 50)
    print(chinese_masked)
    
    # NER è­˜åˆ¥
    ner = NamedEntityRecognizer()
    ner_en_results = ner.recognize_english(english_masked)
    ner_zh_results = ner.recognize_chinese(chinese_masked)
    merged_ner_results = ner.merge_results(ner_en_results, ner_zh_results)
    
    pprint(merged_ner_results)
