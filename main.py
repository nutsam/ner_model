import re
import spacy
from pprint import pprint
from collections import defaultdict
from ckip_transformers.nlp import CkipNerChunker

class TextPreprocessor:
    def __init__(self, stopwords=None, remove_punctuation=True):
        self.stopwords = stopwords or set()
        self.remove_punctuation = remove_punctuation
    
    def preprocess(self, text):
        """
        é€²è¡Œæ–‡æœ¬å‰è™•ç†ï¼š
        1. ç§»é™¤ç¶²å€
        2. ç§»é™¤ HTML æ¨™ç±¤
        3. ç§»é™¤ç‰¹æ®Šå­—å…ƒï¼ˆå¯é¸æ˜¯å¦ä¿ç•™æ¨™é»ç¬¦è™Ÿï¼‰
        4. çµ±ä¸€ç©ºç™½
        5. ç§»é™¤åœç”¨è©
        """
        text = re.sub(r'(http|ftp|https)://[^\s]+', '', text)  # ç§»é™¤ç¶²å€
        text = re.sub(r'<[^>]*>', '', text)  # ç§»é™¤ HTML æ¨™ç±¤
        
        if self.remove_punctuation:
            text = re.sub(r'[^\u4e00-\u9fffA-Za-z0-9\s]', '', text)
        
        text = re.sub(r'\s+', ' ', text).strip()  # çµ±ä¸€ç©ºç™½
        
        if self.stopwords:
            text = ' '.join(word for word in text.split() if word not in self.stopwords)
        
        return text

class TextMasker:
    @staticmethod
    def replace_chinese_with_underscores(text):
        """å°‡ä¸­æ–‡æ›¿æ›ç‚ºç›¸åŒé•·åº¦çš„åº•ç·š"""
        pattern = re.compile(r'[\u4e00-\u9fff0-9]+')
        return pattern.sub(lambda match: '_' * len(match.group(0)), text).replace('\n', '')

    @staticmethod
    def replace_english_with_underscores(text):
        """å°‡è‹±æ–‡å–®å­—æ›¿æ›ç‚ºç›¸åŒé•·åº¦çš„åº•ç·š"""
        pattern = re.compile(r'[A-Za-z]+')
        return pattern.sub(lambda match: '_' * len(match.group(0)), text).replace('\n', '')

class NamedEntityRecognizer:
    def __init__(self):
        self.ner_types = {"CARDINAL", "DATE", "EVENT", "FAC", "GPE", "LANGUAGE", "LAW",
                          "LOC", "MONEY", "NORP", "ORDINAL", "ORG", "PERCENT", "PERSON",
                          "PRODUCT", "QUANTITY", "TIME", "WORK_OF_ART"}
        
        # è¼‰å…¥ spaCy è‹±æ–‡æ¨¡å‹
        self.nlp_en = spacy.load("en_core_web_sm")
        self.nlp_en.add_pipe("gliner_spacy", config={"labels": list(self.ner_types)})
        
        # è¼‰å…¥ CKIP ä¸­æ–‡æ¨¡å‹
        self.nlp_zh = CkipNerChunker(model="bert-base")

    def recognize_english(self, text):
        doc = self.nlp_en(text)
        ner_dict = {ner: [] for ner in self.ner_types}
        
        for ent in doc.ents:
            cleaned_text = ent.text.strip().strip('_')
            if cleaned_text and ent.text == cleaned_text and ent.label_ in self.ner_types:
                ner_dict[ent.label_].append(cleaned_text)
        
        return ner_dict

    def recognize_chinese(self, text):
        result = self.nlp_zh([text])
        ner_dict = {ner: [] for ner in self.ner_types}
        
        for token in result[0]:
            cleaned_word = token.word.strip().strip('_')
            if cleaned_word and token.word == cleaned_word:
                ner_dict[token.ner].append(cleaned_word)
        
        return ner_dict

    def merge_results(self, ner_en_dict, ner_zh_dict):
        merged_dict = defaultdict(list)
        for d in [ner_en_dict, ner_zh_dict]:
            for key, value in d.items():
                merged_dict[key].extend(value)
        return dict(merged_dict)

if __name__ == "__main__":
    text = """
    æ˜¨å¤©ï¼ˆ2024/02/20ï¼‰ï¼Œåœ¨æ±äº¬Skytreeèˆ‰è¾¦äº†ä¸€å ´åç‚ºã€ŒAI Summit 2024ã€çš„ç§‘æŠ€è«–å£‡ï¼Œå¸å¼•äº†ä¾†è‡ª ğ•Šğ•€ğ•ƒğ•€ğ•”ğ•†ğ•Ÿ ğ•ğ•’ğ•ğ•ğ•–ğ•ª çš„å°ˆå®¶åƒèˆ‡ã€‚æ“šå ±å°ï¼Œä¾†è‡ª OpenAIã€DeepMind å’Œ ğ”¹ğ•’ğ•šğ••ğ•¦ çš„å·¥ç¨‹å¸«è¨è«–äº†äººå·¥æ™ºæ…§çš„æœªä¾†è¶¨å‹¢ï¼Œå…¶ä¸­ä¸€é …ä¸»é¡Œæ˜¯é—œæ–¼ LLM èƒ½å¦å–ä»£äººé¡å‰µé€ åŠ›ï¼Ÿ
    ğŸŒ è©²æ´»å‹•ç”± NVIDIA å’Œ TSMC (å°ç©é›») è¯åˆè´ŠåŠ©ï¼Œä¸¦ä¸”å¸å¼•äº†åŒ…æ‹¬ Microsoft åŠ ğ’œğ“…ğ“…ğ“ğ‘’ åœ¨å…§çš„ä¼æ¥­ä»£è¡¨ç™¼è¡¨æ¼”è¬›ã€‚ä¾†è‡ªæ–°åŠ å¡çš„å­¸è€…é™³åšå£«ï¼ˆDr. Chenï¼‰æåˆ°ï¼šã€ŒAI åœ¨ 2024 å¹´çš„ç™¼å±•å°‡åŠ é€Ÿï¼Œä½†æ³•è¦ï¼ˆGDPR & AI Actï¼‰ä»éœ€é€²ä¸€æ­¥å®Œå–„ã€‚ã€
    ğŸ“Œ æ ¹æ“šæ•¸æ“šå ±å‘Šï¼Œ2023 å¹´ AI ç”¢æ¥­çš„ç¸½å€¼é”åˆ° $15.7B USDï¼ŒåŒæ¯”å¢é•· 22.5%ï¼Œå…¶ä¸­ OpenAI çš„ GPT æ¨¡å‹å·²æ“æœ‰ è¶…é 1.2 å„„ ç”¨æˆ¶ã€‚å¸‚å ´åˆ†æå…¬å¸ Gartner é æ¸¬ï¼Œ2025 å¹´ AI å¸‚å ´å°‡é”åˆ° $50Bã€‚
    ğŸ’° åŒæ™‚ï¼Œæ—¥æœ¬æ”¿åºœå®£å¸ƒå°‡æŠ•è³‡ Â¥100å„„ æ—¥åœ“æ–¼ AI ç ”ç©¶ï¼Œä¸¦è¨ˆåŠƒåœ¨æ±äº¬éƒ½å…§å»ºç«‹æ–°çš„ AI ç ”ç©¶ä¸­å¿ƒï¼ˆTokyo AI Research Centerï¼‰ã€‚å¦å¤–ï¼Œä¸­åœ‹çš„ã€Œå¤©å•äºŒè™Ÿã€æ¢æ¸¬å™¨è¨ˆåŠƒæ–¼ 2026 å¹´ ç™»é™¸ç«æ˜Ÿï¼Œé€™èˆ‡ NASA çš„ Artemis è¨ˆç•«å½¢æˆç«¶çˆ­ã€‚
    ğŸ”— æ›´å¤šè³‡è¨Šè«‹åƒè€ƒï¼šhttps://www.aisummit2024.com/event?id=xyz_1234 æˆ–è€…ç™¼é€ Email è‡³ info@aisummit.com ğŸ“§ã€‚
    """
    
    # æ–‡æœ¬å‰è™•ç†
    preprocessor = TextPreprocessor()
    cleaned_text = preprocessor.preprocess(text)
    
    # æ–‡å­—é®ç½©
    english_masked = TextMasker.replace_chinese_with_underscores(cleaned_text)
    chinese_masked = TextMasker.replace_english_with_underscores(cleaned_text)
    
    # NER è­˜åˆ¥
    ner = NamedEntityRecognizer()
    ner_en_results = ner.recognize_english(english_masked)
    ner_zh_results = ner.recognize_chinese(chinese_masked)
    merged_ner_results = ner.merge_results(ner_en_results, ner_zh_results)
    
    pprint(merged_ner_results)
